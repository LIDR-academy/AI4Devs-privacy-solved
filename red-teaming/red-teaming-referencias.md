# Referencias sobre Red-Teaming

A continuación, algunas fuentes clave sobre la técnica de Red-Teaming aplicada en LLMs:

1. **Confident AI: Guía sobre Red-Teaming en LLMs**  
   Enlace: [Confident AI - LLM Red-Teaming Guide](https://www.confident-ai.com/llm-red-teaming)  
   Descripción: Un recurso exhaustivo sobre cómo realizar Red-Teaming en modelos de IA para detectar vulnerabilidades antes de lanzarlos al público.

2. **OpenAI Research: Simulaciones de ataques en IA**  
   Enlace: [OpenAI Research - AI Attacks](https://openai.com/research)  
   Descripción: Explicación detallada de cómo OpenAI utiliza Red-Teaming para probar la seguridad de sus modelos, incluyendo ejemplos prácticos.

3. **Google: Implementación de Red-Teaming en Gemini**  
   Enlace: [Google AI - Gemini Red-Teaming](https://ai.google/research)  
   Descripción: Descripción de cómo Google lleva a cabo simulaciones de ataques para garantizar que sus modelos IA sean resistentes a posibles exploits.
